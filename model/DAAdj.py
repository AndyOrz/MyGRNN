import torch
import torch.nn as nn
import torch.nn.functional as F


class DAAdj(nn.Module):
	def __init__(self, num_feats, num_heads:int, negative_slope:float=0):
		"""
		Dynamic Attention Adjacency matrix
		åŠ¨æ€æ³¨æ„åŠ›é‚»æ¥é˜µ
		æ ¹æ®èŠ‚ç‚¹ç‰¹å¾å»ºç«‹å¸¦æƒé‡çš„é‚»æ¥çŸ©é˜µ

		å‚æ•°ï¼š
		num_feats	è¾“å…¥ç»´æ•°
		num_heads	æ³¨æ„åŠ›å¤´æ•°
		negative_slope LeakyReLUçš„è´ŸåŠè½´æ–œç‡
		"""
		super(DAAdj,self).__init__()
		self.num_heads = num_heads
		self.num_feats = num_feats
		self.lin_dist = nn.Linear(num_feats * 2, num_heads)
		self.lin_merge = nn.Linear(num_heads, 1)
		if negative_slope == 0:
			self.leaky_relu = nn.ReLU()
		else:
			self.leaky_relu = nn.LeakyReLU(negative_slope)
	
		self.selfbias = nn.Parameter(torch.ones(num_heads,1))

		
	def forward(self, x): 
		"""
		å‘å‰ä¼ æ’­

		å‚æ•°ï¼š
		x [num_nodes, num_feats] è¾“å…¥
		
		è¾“å‡º é‚»æ¥çŸ©é˜µ [num_nodes, num_nodes]
		"""
		num_nodes, num_feats= x.shape

        # [num_nodes, num_nodes, num_feats]
		x_s = x.unsqueeze(1).repeat(1,num_nodes,1) #repeat alone axis 1
		x_t = x.unsqueeze(0).repeat(num_nodes,1,1) #repeat alone axis 0

		#æŸç§æ„ä¹‰ä¸Šæ˜¯è¡¨ç¤ºvehicle ğ‘— ä¸ vehicle ğ‘–çš„åµŒå…¥å·®å¼‚ï¼ˆi->jï¼‰
		dist = self.lin_dist(torch.cat((x_s,x_t),2))
		dist += torch.diag_embed(self.selfbias.repeat(1, num_nodes), dim1=0, dim2=1)
		#å¤šå¤´æ³¨æ„åŠ› [num_nodes, num_nodes, num_heads]
		heads = F.softmax(self.leaky_relu(dist),1)
		#èåˆæ³¨æ„åŠ› [num_nodes, num_nodes, 1]
		return self.lin_merge(heads)[...,0]