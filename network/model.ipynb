{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "996d797ae47d9163ed8fc89b93d4b0da9b0a46e739f34cf0a3d45094f71fb8f7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn import GATConv"
   ]
  },
  {
   "source": [
    "### GRUCell"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图卷积GRU单元\n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, **kwargs):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "        input_size  输入尺寸\n",
    "        hidden_size 输出尺寸/隐藏尺寸\n",
    "        **kwargs    传递给图卷积层的其它参数\n",
    "        若是GAT，则至少还有num_heads\n",
    "\n",
    "        输出与隐藏状态之间经过线性变换\n",
    "        \"\"\"\n",
    "        super(GRUCell, self).__init__() #父类初始化函数\n",
    "        #生成卷积层\n",
    "        self.ConvIR = GATConv(input_size + hidden_size, hidden_size, **kwargs)\n",
    "        self.ConvIZ = GATConv(input_size + hidden_size, hidden_size, **kwargs)\n",
    "        self.ConvIH = GATConv(input_size + hidden_size, hidden_size, **kwargs)\n",
    "\n",
    "\n",
    "    def forward(self, g, input, hx):\n",
    "        \"\"\"\n",
    "        Gated recurrent unit (GRU) with Graph Convolution.\n",
    "        图卷积改进的GRU\n",
    "\n",
    "        参数：\n",
    "        g           dgl.graph           图结构\n",
    "        input       [N, num_feats]      输入\n",
    "        hx          [N, hidden_size]    隐藏状态(不能为空)\n",
    "\n",
    "        输出:\n",
    "        [N, hidden_size]\n",
    "        输出值/隐藏状态\n",
    "        \"\"\"\n",
    "\n",
    "        #连接输入和隐藏状态 => [N, num_feats+hidden_size]\n",
    "        inputvalue = torch.cat([input, hx],dim=1)\n",
    "\n",
    "        r = torch.sigmoid(self.ConvIR(g, inputvalue)) #重置门\n",
    "        z = torch.sigmoid(self.ConvIZ(g, inputvalue)) #更新门\n",
    "\n",
    "        h = torch.tanh(self.ConvIH(g, torch.cat([input, r * hx], dim=1))) #新记忆\n",
    "\n",
    "        new_state = z * hx + (1.0 - z) * h #融合新记忆和旧记忆\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "source": [
    "## Encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, num_layers:int, **kwargs):\n",
    "        \"\"\"\n",
    "        使用GRUCell的编码器\n",
    "\n",
    "        参数：\n",
    "        input_size       输入维数\n",
    "        hidden_size      隐藏状态尺寸\n",
    "        num_layers       GRU层数\n",
    "        **kwargs         GRU的其他参数\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        #除了第一层输入维数为input_size，输出维数为hidden_size\n",
    "        #之后的每一层的GRU的输入输出维数相同为hidden_size\n",
    "        #所有GRU上一层的输出作为下一层的输入，隐藏状态来自自身的上一时刻输出\n",
    "        self.GRUs = nn.ModuleList([ GRUCell(input_size, hidden_size, **kwargs) if i == 0\n",
    "                                   else GRUCell(hidden_size, hidden_size, **kwargs) \n",
    "                                   for i in range(self._num_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, g, input, hx=None):\n",
    "        \"\"\"\n",
    "        每一层GRU以上一层GRU的输出作为输入，\n",
    "        每一时刻输入获得的GRU的隐藏状态传递给同一GRU的下一时刻\n",
    "        参数：\n",
    "        g           dgl.graph                       图结构\n",
    "        input       [N, num_feats]                  输入值\n",
    "        hx          [num_layers, N, hidden_size]    隐藏状态（多出一维是因为每一层GRU都有一个隐藏状态）\n",
    "\n",
    "        输出\n",
    "        [num_layers, N, hidden_size]\n",
    "        \"\"\"\n",
    "        #对序列首个输入进行初始化\n",
    "        if hx is None:\n",
    "            hx = input.new_zeros((self._num_layers, input.shape[0], self._hidden_size))\n",
    "\n",
    "        hidden_states = []\n",
    "        output = input\n",
    "\n",
    "        #以上一层的输出为下一层输入\n",
    "        #每个GRU的隐藏状态来自自身的上一时刻输出\n",
    "        for i, gru in enumerate(self.GRUs):\n",
    "            next_hidden_state = gru(g, output, hx[i])\n",
    "            hidden_states.append(next_hidden_state)\n",
    "            output = next_hidden_state\n",
    "\n",
    "        return torch.stack(hidden_states)\n"
   ]
  },
  {
   "source": [
    "## Decoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size:int, output_size:int, num_layers:int, proj:bool=True, proj_settings=None ,**kwargs):\n",
    "        \"\"\"\n",
    "        上一时刻预测结果将作为本次预测的输入，所以输入输出必须等大小\n",
    "        参数：\n",
    "        hidden_size      隐藏状态尺寸\n",
    "        output_size      输出维数\n",
    "        num_layers       GRU层数\n",
    "        proj             是否进行投影，仅在隐藏尺寸和输出维数相同时可以不投影\n",
    "        proj_settings    投影层设置，字典形式。若proj参数为False将被忽略\n",
    "        **kwargs         GRU的其他参数\n",
    "        \"\"\"\n",
    "        super(Decoder,self).__init__()\n",
    "        assert hidden_size==output_size or proj, \"若不使用投影层，输出维度与隐藏状态尺寸必须保持一致。\"\n",
    "        self._num_layers = num_layers\n",
    "        self._output_size = output_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "\n",
    "        #除了第一层输入维数为output_size，输出维数为hidden_size\n",
    "        #之后的每一层的GRU的输入输出维数相同为hidden_size\n",
    "        #所有GRU上一层的输出作为下一层的输入，隐藏状态来自自身的上一时刻输出\n",
    "        self.GRUs = nn.ModuleList([ GRUCell(output_size, hidden_size, **kwargs) if i == 0\n",
    "                                   else GRUCell(hidden_size, hidden_size, **kwargs)\n",
    "                                   for i in range(self._num_layers)])\n",
    "        \n",
    "        self.projection_layer = nn.Linear(self._hidden_size, self._output_size, **proj_settings) if proj else None\n",
    "\n",
    "    def forward(self, g, input, hx):\n",
    "        \"\"\"\n",
    "        隐藏状态来自同层Encoder最后一个GRU的，最后一次隐藏状态\n",
    "        参数：\n",
    "        g           dgl.graph                       图结构\n",
    "        input       [N, output_dim]                 输入值\n",
    "        hx          [num_layers, N, hidden_size]    隐藏状态（不能为空）\n",
    "\n",
    "        输出：\n",
    "        预测值，隐藏状态\n",
    "        [N, output_size], [num_layers, N, hidden_size]\n",
    "        \"\"\"\n",
    "        hidden_states = []\n",
    "\n",
    "        output = input\n",
    "        for i, gru in enumerate(self.GRUs):\n",
    "            next_hidden_state = gru(g, output, hx[i])\n",
    "            hidden_states.append(next_hidden_state)\n",
    "            output = next_hidden_state\n",
    "\n",
    "        #投影\n",
    "        if self.projection_layer is not None:\n",
    "            output = self.projection_layer(output)\n",
    "\n",
    "        return output, torch.stack(hidden_states)"
   ]
  },
  {
   "source": [
    "# MyModel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq 模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,num_feats:int, output_dim:int, hidden_size:int, num_layers:int,\n",
    "                 seq_len:int, horizon:int, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化Encoder-Decoder模型\n",
    "        输入数据参数：\n",
    "        num_feats    输入维数\n",
    "        output_dim  输出维数\n",
    "\n",
    "        预测需求参数：\n",
    "        seq_len     输入序列长度\n",
    "        horizon     输出序列长度\n",
    "\n",
    "        超参数：\n",
    "        hidden_size     隐藏状态尺寸\n",
    "        num_layers      网络层数\n",
    "\n",
    "        **kwargs        其他参数\n",
    "        \"\"\"\n",
    "        super(MyModel,self).__init__() #父类初始化函数\n",
    "\n",
    "        self._num_feats = num_feats                  #输入参数\n",
    "        self._output_dim = output_dim                #输出参数\n",
    "\n",
    "        self._num_layers = num_layers                #超参数-网络层数\n",
    "        self._hidden_size = hidden_size              #超参数-记忆尺度\n",
    "        \n",
    "        #模块\n",
    "        self.encoder_model = Encoder(input_size = self._num_feats,\n",
    "                                     hidden_size = self._hidden_size,\n",
    "                                     num_layers = self._num_layers, **kwargs)\n",
    "\n",
    "        self.decoder_model = Decoder(output_size = self._output_dim,\n",
    "                                     hidden_size = self._hidden_size,\n",
    "                                     num_layers = self._num_layers, **kwargs)\n",
    "        #可能根据预测需求变化的参数\n",
    "        self.seq_len = seq_len \n",
    "        self.horizon = horizon \n",
    "\n",
    "\n",
    "    def forward(self, g, x, y=None, p=0):\n",
    "        \"\"\"\n",
    "        向前传播\n",
    "\n",
    "        参数：\n",
    "        g dgl.graph\n",
    "        x [seq_len, N ,num_feats] 输入序列\n",
    "        y [horizon, N ,num_feats] 输出序列的真值（可选）\n",
    "        p 在decode阶段给予真值的概率（默认：0）\n",
    "\n",
    "        输出：\n",
    "        torch.tensor\n",
    "        [horizon, N, output_dim] 预测时间长度 * 节点数 * 输出属性数\n",
    "        \"\"\"\n",
    "        #N是可变的，正常输入为节点数，当使用mini-batch时为节点数*batch_size\n",
    "        self._N = x.shape[1]\n",
    "        assert g.num_nodes() == self._N, \"参数 g 和 x 中的节点数量不同\" #debug\n",
    "\n",
    "        #编码\n",
    "        encoder_hidden_state = self.encode(g, x)\n",
    "        #解码\n",
    "        outputs = self.decode(g, encoder_hidden_state, truth=y, p=p) #可以考虑采用最后一个输入作为启动\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def encode(self, g, x):\n",
    "        \"\"\"\n",
    "        将时间序列，逐个输入编码网络中\n",
    "        参数：\n",
    "        g dgl.graph\n",
    "        x [seq_len, N ,num_feats] 输入序列\n",
    "\n",
    "        输出\n",
    "        torch.tensor\n",
    "        [num_layers, N, hidden_size]\n",
    "\n",
    "        \"\"\"\n",
    "        encoder_hidden_state = None\n",
    "        for t in range(self.seq_len):\n",
    "            encoder_hidden_state = self.encoder_model(g, x[t], encoder_hidden_state)\n",
    "            \n",
    "        return encoder_hidden_state\n",
    "\n",
    "\n",
    "    def decode(self, g, encoder_hidden_state, startup_seq=None, truth=None, p=0):\n",
    "        \"\"\"\n",
    "        逐个生成序列\n",
    "        参数：\n",
    "        g dgl.graph\n",
    "        encoder_hidden_state [num_layers, N, hidden_size] 编码结果\n",
    "        startup_seq          [N, output_dim]              启动值（默认值：全0张量）\n",
    "\n",
    "        Curriculum Learning参数（仅在训练时使用）\n",
    "        truth   [horizon, N, output_dim]    真值\n",
    "        p       给予真值的概率\n",
    "\n",
    "        输出 [horizon, N, output_dim]\n",
    "        \"\"\"\n",
    "        #若未指定启动序列，则输入全0\n",
    "        #dtype和device与encoder_hidden_state相同\n",
    "        decoder_input = encoder_hidden_state.new_zeros((self._N, self._output_dim)) if startup_seq is None else startup_seq\n",
    "\n",
    "        decoder_hidden_state = encoder_hidden_state\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "            decoder_input, decoder_hidden_state = self.decoder_model(g, decoder_input, decoder_hidden_state)\n",
    "            outputs.append(decoder_input)\n",
    "            #以一定概率给予真值\n",
    "            if self.training and truth is not None:\n",
    "                if np.random.uniform(0, 1) < p:\n",
    "                    decoder_input = truth[t]\n",
    "\n",
    "        return torch.stack(outputs)"
   ]
  }
 ]
}